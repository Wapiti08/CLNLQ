
Cover the 


from sklearn.pipeline import Pipeline
from spacy import tokenizer
from autocorrect import Speller
import speech_recognition as sr


def type_check():

def tokenize_words():
    # check spelling, default English
    # not necessary for Chinese
    spell = Speller(lang='en')


    # tokenize to NLQ semantic roles

    # give Ids to each token

def token_process():
    # split/lemmatize tokens to roots


def max_length():



def token_label():
